{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7ta2914pOxLq5AxcSRRfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gautamaero/gautamaero/blob/main/Copy_of_different_NE_tagger_ZH_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different NE tagger for chienese langauge:"
      ],
      "metadata": {
        "id": "4Nd4Y6Ebxs_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy jieba --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFYU9-4UyP00",
        "outputId": "1194a2aa-1249-47b4-e78c-d4d56f185ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download zh_core_web_sm --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwx7GhgSyXk9",
        "outputId": "39582b25-b7b1-411b-ae84-97ff2279ed39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q83zxKKXxnYu"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import jieba\n",
        "\n",
        "# Load the Chinese language model for SpaCy\n",
        "nlp = spacy.load(\"zh_core_web_sm\")\n",
        "\n",
        "# Function to perform NER using SpaCy for Chinese sentences\n",
        "def ner_for_chinese_sentences_spacy(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    all_entities = []\n",
        "    individual_entities = []\n",
        "\n",
        "    for sentence_str in sentences:\n",
        "        # Tokenize the sentence using Jieba\n",
        "        words = jieba.cut(sentence_str.strip())\n",
        "        segmented_sentence = \" \".join(words)\n",
        "\n",
        "        # Process the segmented sentence with SpaCy\n",
        "        doc = nlp(segmented_sentence)\n",
        "\n",
        "        # Extract named entities from the processed sentence\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        # Append entities to the list for all sentences\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "        # Append entities to the list for individual sentences\n",
        "        individual_entities.append(entities)\n",
        "\n",
        "    return all_entities, individual_entities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File path for Chinese sentences\n",
        "chinese_file = \"/content/train_2000000.zh\"\n",
        "# Execute NER for Chinese sentences using SpaCy\n",
        "all_entities_spacy, individual_entities_spacy = ner_for_chinese_sentences_spacy(chinese_file)\n",
        "\n",
        "# Write all entities to a file (one output per line)\n",
        "with open(\"ZH_NE_tag_Spacy_2000000_sentence_NEtAG.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entity in all_entities_spacy:\n",
        "        file.write(f\"{entity[0]}: {entity[1]}\\n\")\n",
        "\n",
        "# Write entities for each line to a file\n",
        "with open(\"ZH_NE_tag_Spacy_2000000_NEtag_Only.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entities in individual_entities_spacy:\n",
        "        for entity in entities:\n",
        "            file.write(f\"{entity[0]}: {entity[1]}, \")\n",
        "        file.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "Kq6T6eJxx2Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK library for NER tagger for Chinese langauges:\n",
        "\n",
        "`\n",
        "NLTK does not directly support Chinese language processing out-of-the-box. However, you can use other libraries for Chinese text processing along with NLTK for Named Entity Recognition (NER). One such library is Jieba for Chinese word segmentation.`"
      ],
      "metadata": {
        "id": "WeAwHrhQ3rKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import jieba\n",
        "\n",
        "# Download NLTK resources (needed for POS tagging)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to perform NER using NLTK for Chinese sentences\n",
        "def ner_for_chinese_sentences_nltk(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    all_entities = []\n",
        "    individual_entities = []\n",
        "\n",
        "    for sentence_str in sentences:\n",
        "        # Tokenize the sentence using Jieba\n",
        "        words = jieba.cut(sentence_str.strip())\n",
        "\n",
        "        # Perform part-of-speech tagging using NLTK\n",
        "        pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "        # Extract named entities based on POS tags\n",
        "        entities = [(word, tag) for word, tag in pos_tags if tag.startswith('NN')]\n",
        "\n",
        "        # Append entities to the list for all sentences\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "        # Append entities to the list for individual sentences\n",
        "        individual_entities.append(entities)\n",
        "\n",
        "    return all_entities, individual_entities\n"
      ],
      "metadata": {
        "id": "0F6WTjf_3xUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File path for Chinese sentences\n",
        "chinese_file = \"/content/train_2000000.zh\"\n",
        "\n",
        "# Execute NER for Chinese sentences using NLTK\n",
        "all_entities_nltk, individual_entities_nltk = ner_for_chinese_sentences_nltk(chinese_file)\n",
        "\n",
        "# Write all entities to a file (one output per line)\n",
        "with open(\"ZH_NE_tag_NLT-JIEBA_2000000_sentence_NEtAG.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entity in all_entities_nltk:\n",
        "        file.write(f\"{entity[0]}: {entity[1]}\\n\")\n",
        "\n",
        "# Write entities for each line to a file\n",
        "with open(\"ZH_NE_tag_NLT-JIEBA_2000000_NEtag_Only.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entities in individual_entities_nltk:\n",
        "        for entity in entities:\n",
        "            file.write(f\"{entity[0]}: {entity[1]}, \")\n",
        "        file.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "-ythvl_D4FHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We tokenize Chinese sentences using Jieba.\n",
        "* We perform part-of-speech (POS) tagging using  NLTK's pos_tag function on the tokenized words.\n",
        "* We extract named entities based on POS tags. Here, we assume that nouns (tags starting with 'NN') correspond to named entities.\n",
        "* We write the extracted entities to two different text files, following the specified format."
      ],
      "metadata": {
        "id": "6LIL4hGg4pGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standford Parser:"
      ],
      "metadata": {
        "id": "9Vg2e3hr5FCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "import os\n",
        "\n",
        "# Set the path to the Stanford NER Tagger JAR file and the model file\n",
        "stanford_ner_path = '/path/to/stanford-ner-YYYY-MM-DD'  # Adjust this path\n",
        "jar_file = os.path.join(stanford_ner_path, 'stanford-ner.jar')\n",
        "model_file = os.path.join(stanford_ner_path, 'chinese.misc.distsim.crf.ser.gz')\n",
        "\n",
        "# Initialize Stanford NER Tagger\n",
        "ner_tagger = StanfordNERTagger(model_file, jar_file, encoding='utf-8')\n",
        "\n",
        "# Function to perform NER using Stanford NER Tagger for Chinese sentences\n",
        "def ner_for_chinese_sentences_stanford(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    all_entities = []\n",
        "    individual_entities = []\n",
        "\n",
        "    for sentence_str in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        words = sentence_str.strip().split()\n",
        "\n",
        "        # Perform NER using Stanford NER Tagger\n",
        "        entities = ner_tagger.tag(words)\n",
        "\n",
        "        # Filter out entities with 'O' tag\n",
        "        entities = [(word, tag) for word, tag in entities if tag != 'O']\n",
        "\n",
        "        # Append entities to the list for all sentences\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "        # Append entities to the list for individual sentences\n",
        "        individual_entities.append(entities)\n",
        "\n",
        "    return all_entities, individual_entities\n",
        "\n"
      ],
      "metadata": {
        "id": "gJbBIGCX4zSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# File path for Chinese sentences\n",
        "chinese_file = \"/content/train_2000000.zh\"\n",
        "\n",
        "# Execute NER for Chinese sentences using Stanford NER Tagger\n",
        "all_entities_stanford, individual_entities_stanford = ner_for_chinese_sentences_stanford(chinese_file)\n",
        "\n",
        "# Write all entities to a file (one output per line)\n",
        "with open(\"ZH_NE_tag_Standford_2000000_sentence_NEtAG.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entity in all_entities_stanford:\n",
        "        file.write(f\"{entity[0]}: {entity[1]}\\n\")\n",
        "\n",
        "# Write entities for each line to a file\n",
        "with open(\"ZH_NE_tag_Standford_2000000_NEtag_Only.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entities in individual_entities_stanford:\n",
        "        for entity in entities:\n",
        "            file.write(f\"{entity[0]}: {entity[1]}, \")\n",
        "        file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "WVyKyBkn5T1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flair: Tagger for Chinese langauge\n"
      ],
      "metadata": {
        "id": "t_AjTlsd6LTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# Load the Flair NER tagger for Chinese\n",
        "tagger = SequenceTagger.load('ner-fast')\n",
        "\n",
        "# Function to perform NER using Flair for Chinese sentences\n",
        "def ner_for_chinese_sentences_flair(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    all_entities = []\n",
        "    individual_entities = []\n",
        "\n",
        "    for sentence_str in sentences:\n",
        "        # Create a Flair Sentence\n",
        "        sentence = Sentence(sentence_str.strip())\n",
        "\n",
        "        # Predict NER tags\n",
        "        tagger.predict(sentence)\n",
        "\n",
        "        # Extract named entities\n",
        "        entities = [(entity.text, entity.labels[0].value) for entity in sentence.get_spans('ner')]\n",
        "\n",
        "        # Append entities to the list for all sentences\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "        # Append entities to the list for individual sentences\n",
        "        individual_entities.append(entities)\n",
        "\n",
        "    return all_entities, individual_entities"
      ],
      "metadata": {
        "id": "Iq9YXHlG6bAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# File path for Chinese sentences\n",
        "chinese_file = \"/content/train_2000000.zh\"\n",
        "# Execute NER for Chinese sentences using Flair\n",
        "all_entities_flair, individual_entities_flair = ner_for_chinese_sentences_flair(chinese_file)\n",
        "\n",
        "# Write all entities to a file (one output per line)\n",
        "with open(\"ZH_NE_tag_FLAIR_2000000_sentence_NEtAG.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entity in all_entities_flair:\n",
        "        file.write(f\"{entity[0]}: {entity[1]}\\n\")\n",
        "\n",
        "# Write entities for each line to a file\n",
        "with open(\"ZH_NE_tag_FLAIR_2000000_NEtag_Only.txt\", 'w', encoding='utf-8') as file:\n",
        "    for entities in individual_entities_flair:\n",
        "        for entity in entities:\n",
        "            file.write(f\"{entity[0]}: {entity[1]}, \")\n",
        "        file.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "XfJRlb0s6c02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}